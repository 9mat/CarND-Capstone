{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- Keras\n",
    "- Tensorflow\n",
    "- Tensorflow tools: freeze_graph, optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input, Convolution2D, MaxPooling2D, Activation, concatenate, warnings\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import get_file\n",
    "from keras.utils import layer_utils\n",
    "from keras.applications.xception import Xception\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "As the first step, I will use classification instead of object detection (i.e. ignore the case where multiple traffic lights of different colors are present). \n",
    "SqueezeNet is recommended by [David Brailovsky](https://medium.freecodecamp.org/recognizing-traffic-lights-with-deep-learning-23dae23287cc). The following Keras SqueezeNet is adopted from [rcmalli](https://github.com/rcmalli/keras-squeezenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, warnings\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import get_file\n",
    "from keras.utils import layer_utils\n",
    "\n",
    "\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "WEIGHTS_PATH = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    "WEIGHTS_PATH_NO_TOP = \"https://github.com/jeremykawahara/keras-squeezenet/raw/master/weights/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "# Modular function for Fire Node\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Original SqueezeNet from paper.\n",
    "\n",
    "def SqueezeNet(include_top=True, weights='imagenet',\n",
    "               input_tensor=None, input_shape=None,\n",
    "               pooling=None,\n",
    "               classes=1000):\n",
    "    \"\"\"Instantiates the SqueezeNet architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=227,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "\n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    if include_top:\n",
    "        # It's not obvious where to cut the network... \n",
    "        # Could do the 8th or 9th layer... some work recommends cutting earlier layers.\n",
    "    \n",
    "        x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "        x = Activation('relu', name='relu_conv10')(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Activation('softmax', name='loss')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling=='max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "        elif pooling==None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = Model(inputs, x, name='squeezenet')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models')\n",
    "        else:\n",
    "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models')\n",
    "            \n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "Training data is downloaded from the link on [Vatsal Srivastava's github](https://drive.google.com/file/d/0B-Eiyn-CUQtxdUZWMkFfQzdObUE/view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SqueezeNet(weights='imagenet', include_top=False, input_shape = (300,400,3))\n",
    "\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "predictions = Dense(4, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "train_data_dir = './data/data/sim_training_data/sim_data_capture'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(\n",
    "# rescale = 1./255,\n",
    "# horizontal_flip = True,\n",
    "# fill_mode = \"nearest\",\n",
    "# zoom_range = 0.3,\n",
    "# width_shift_range = 0.3,\n",
    "# height_shift_range=0.3,\n",
    "# rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (300, 400),\n",
    "batch_size = 4, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "# validation_data_dir,\n",
    "# target_size = (img_height, img_width),\n",
    "# class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = 256,\n",
    "epochs = 20)\n",
    "# validation_data = validation_generator,\n",
    "# nb_val_samples = nb_validation_samples,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = K.get_session()\n",
    "gd = sess.graph.as_graph_def()\n",
    "saver = tf.train.Saver()\n",
    "tf.train.write_graph(gd, '.', 'model.pb', as_text=True)\n",
    "saver.save(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "savefolder = '.'\n",
    "input_graph_path = 'model.pb'\n",
    "checkpoint_path = 'model.ckpt'\n",
    "input_saver_def_path = \"\"\n",
    "input_binary = False\n",
    "output_node_names = predictions.name.split(':')[0]\n",
    "restore_op_name = \"save/restore_all\" # deprecated, unused\n",
    "filename_tensor_name = \"save/Const:0\" # deprecated, unused\n",
    "output_frozen_graph_name = 'frozen_graph.pb'\n",
    "output_optimized_graph_name = 'optimized_graph.pb'\n",
    "clear_devices = True\n",
    "\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                          input_binary, checkpoint_path, output_node_names,\n",
    "                          restore_op_name, filename_tensor_name,\n",
    "                          output_frozen_graph_name, clear_devices, \"\")\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                            \n",
    "# Optimize for inference\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open(output_frozen_graph_name, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "        input_graph_def,\n",
    "        [model.input.name.split(':')[0]], # an array of the input node(s)\n",
    "        [predictions.name.split(':')[0]], # an array of output nodes\n",
    "        tf.float32.as_datatype_enum)\n",
    "\n",
    "# Save the optimized graph\n",
    "tf.train.write_graph(output_graph_def, savefolder, output_optimized_graph_name, as_text=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = model.input.name\n",
    "output_name = predictions.name\n",
    "io_name = {'input_name': input_name, 'output_name': output_name}\n",
    "import json\n",
    "with open('tensor_name.json', 'w') as f:\n",
    "    json.dump(io_name, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
